---
layout: page #program
---

# Program

<br>
The workshop will take place on Monday, December 13, 2021.
Please note that all times are in Eastern Standard Time (New York).

Information about invited speakers can be found <a href="{{ site.baseurl }}/speakers">here</a>. 

<table class="styled-table">
    <thead>
        <tr>
            <th></th>
            <th>Time (EST)</th>
            <th>Event</th>
            <th>Speaker</th>
            <th>Platform</th>
        </tr>
    </thead>
    <tbody>
        <tr class="mingle-row">
            <td></td>
            <td>08:45</td>
            <td>Poster preview</td>
            <td>-</td>
            <td>Gather</td>
        </tr>
        <tr>
            <td></td>
            <td>08:55</td>
            <td>Opening remarks</td>
            <td><em>Organizers</em></td>
            <td>Zoom</td>
        </tr>
        <tr class="session-row">
            <td style="text-align:center;" rowspan=5>Session 1</td>
        </tr>
        <tr>
            <td>09:00</td>
            <td>
            Invited talk: "The Neurobiology of Pragmatics"
            <br><br><p style="font-size:0.75rem;">In this presentation I will discuss recent insights into both the time course of pragmatic processing and the key neural infrastructure for inferring speaker meaning from coded meaning. I will show why mirror neurons are not able to handle pragmatic information. In addition, I will present evidence for the role of the Theory of Mind (ToM) network in processing of pragmatic information.</p>
            </td>
            <td>Peter Hagoort</td>
            <td>Zoom</td>
        </tr>
        <tr>
            <td>09:30</td>
            <td>
            Contributed talk: "Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes"
            <br><br><p style="font-size:0.75rem;">Empathy is a complex cognitive ability based on the reasoning of others' affective states. In order to better understand others and express stronger empathy in dialogues, we argue that two issues must be tackled at the same time: (i) identifying which word is the cause for the other's emotion and (ii) reflecting those specific words in the response generation. However, existing approaches for recognizing emotion cause words in text require sub-utterance level annotations, which is demanding. Taking inspiration from social cognition, we leverage a generative estimator to infer emotion cause words from utterances with only emotion labels. We show our approach improves best performing dialogue agents on generating more focused empathetic responses in terms of both automatic and human evaluation.</p>
            </td>
            <td>Hyunwoo Kim, Byeongchang Kim, Gunhee Kim</td>
            <td>Zoom</td>
        </tr>
        <tr>
            <td>09:45</td>
            <td>
            Contributed talk: "Lexical Pragmatics in the Wild: The Case of Complement Coercion"
            <br><br><p style="font-size:0.75rem;">We inspect complement coercion sentences (she finished the coffee or he started a book) as a case study for modeling open-ended pragmatic interpretation. Existing computational work treats complement coercion interpretation as a task of choosing a single best-fit verb (she finished drinking the coffee). We instead present crowdsourcing and modeling data that supports broadening the predicted classes to better capture naturalistic interpretation.</p>
            </td>
            <td>Frederick Gietz, Barend Beekhuizen</td>
            <td>Zoom</td>
        </tr>
        <tr>
            <td>10:00</td>
            <td>
            Invited talk: "Human Production Strategies for Neural Language Generation"
            <br><br><p style="font-size:0.75rem;">Progress on language generation has experienced a huge boost with the advent of large models trained on huge amounts of text. However, this kind of language modelling will only take us that far. Most natural language use is driven by communicative goals and is often grounded both in the conversational context and in extralinguistic information. Can we take inspiration from human production strategies in situated environments to drive forward natural language generation models? I will argue that yes, we can, and present a few examples of recent and ongoing research carried out within my group that follow this research programme.</p>
            </td>
            <td>Raquel Fernández</td>
            <td>Zoom</td>
        </tr>
        <tr class="mingle-row">
            <td></td>
            <td>10:30</td>
            <td>Coffee break / Meet-and-greet #1</td>
            <td>-</td>
            <td>Gather</td>
        </tr>
        <tr style="font-weight:bold;">
            <td></td>
            <td>11:00</td>
            <td>Panel discussion</td>
            <td><em>Invited speakers</em></td>
            <td>Zoom</td>
        </tr>
         <tr class="mingle-row">
            <td></td>
            <td>12:00</td>
            <td>Lunch / Poster session</td>
            <td>-</td>
            <td>Gather</td>
        </tr>
        <tr class="session-row">
            <td style="text-align:center;" rowspan=5>Session 2</td>
        </tr>
        <tr>
            <td>13:30</td>
            <td>
            Invited talk: "The Right Words for the Job: Coordinating on Task-Relevant Conventions via Bayesian Program Learning"
            <br><br><p style="font-size:0.75rem;">In this talk, I'll argue that human-like language use in a variable and non-stationary social environment requires a more radical shift in our models of meaning. People not only rely on pragmatic reasoning to enrich static literal meanings, but flexibly create new literal meanings together to suit the task at hand. In other words, the central computational problem of communication is not simply transmission in context, as in classical formulations, but continual learning within and across social contexts. As a case study, I'll present a physical assembly task where pairs of human participants worked together to reconstruct block towers. We found that human participants rapidly coordinated on new, more abstract language that captured each scene’s underlying structure. Motivated by these findings, we extend recent hierarchical models of convention formation with a Bayesian program learning module. This model suggests a path toward more adaptive language models that are able to 'find the right words for the job' and collaborate with human partners in a wider variety of novel contexts.</p>
            </td>
            <td>Robert Hawkins</td>
            <td>Zoom</td>
        </tr>
        <tr>
            <td>14:00</td>
            <td>
            Contributed talk: "Loopholes: a Window into Value Alignment and the Learning of Meaning"
            <br><br><p style="font-size:0.75rem;">Exploiting a loophole, taking advantage of the ambiguity of language to do what someone says but not what they want, is a familiar facet of fable, law, and everyday life. Engaging with loopholes requires a nuanced understanding of goals, social ambiguity, and value alignment. Scientifically, the development of loopholes can help us better understand human communication, and design better human-AI interactions. However, cognitive research on this behavior remains scarce. A survey of parents reveals that loophole behavior is prevalent, frequent, and diverse in daily parent-child interactions, emerging around ages five to six. A further experiment shows that adults consider loophole behavior as less costly than non-compliance, and children increasingly differentiate loophole behavior from non-compliance from ages four to ten. We discuss the implications and limitations of the current work, together with a proposal for a formal framework for loophole behavior.</p>
            </td>
            <td>Sophie Bridgers, Elena Glassman, Laura Schulz, Tomer Ullman</td>
            <td>Zoom</td>
        </tr>
        <tr>
            <td>14:15</td>
            <td>
            Contributed talk: "Intuitive Image Descriptions are Context-Sensitive"
            <br><br><p style="font-size:0.75rem;">Consumers of image descriptions want them to be context-sensitive, but previous crowdsourced efforts to create text from images have presented the images in isolation. We tested whether untrained crowdworkers naturally take context into account when writing image descriptions by asking them to write descriptions for images that we embedded in the first paragraph of a Wikipedia article. Our analysis shows that the produced descriptions were statistically significantly more likely to reflect contents of the article they were presented with than those of mismatched articles. These findings have implications on the extent and usefulness of training crowdworkers when developing large scale context-sensitive description corpora, as well as the development of deep learning models for automatic description generation.</p>
            </td>
            <td>Shayan Hooshmand, Elisa Kreiss, Christopher Potts</td>
            <td>Zoom</td>
        </tr>
        <tr>
            <td>14:30</td>
            <td>
            Invited talk: "Incorporating Interaction in Models of Language Use"
            <br><br><p style="font-size:0.75rem;">Everyday conversation comes with an important affordance: interaction. Amongst other forms of metacommunication, interaction allows for the use of other-initiated repair: where a receiver signals trouble in understanding a producer's utterance, thereby prompting the producer to repeat or clarify. This phenomenon is ubiquitous in everyday conversation, but its affordance has largely been ignored in computational models of language use and language evolution. In this talk, I explore what happens when we add other-initiated repair to (i) a model of disambiguation in language use, and (ii) a model of the cultural evolution of compositional structure in language. In the first case study, we show that interactive repair may help outsource some of the computational resource demands of pragmatic reasoning to interaction (where disambiguation takes place across multiple turns). In the second case study, we show that interactive repair may play a role in 'locking in' compositional structure over generations in the cultural evolution of language.</p>
            </td>
            <td>Marieke Woensdregt</td>
            <td>Zoom</td>
        </tr>
        <tr class="mingle-row">
            <td></td>
            <td>15:00</td>
            <td>Coffee break / Meet-and-greet #2</td>
            <td>-</td>
            <td>Gather</td>
        </tr>
        <tr class="session-row">
            <td style="text-align:center;" rowspan=5>Session 3</td>
        </tr>
        <tr>
            <td>15:30</td>
            <td>
            Invited talk: "Living in the moment: Studying pragmatic inference with temporally sensitive measures of comprehension"
            <br><br><p style="font-size:0.75rem;">We will take a whirlwind, mile high, tour of the literature on the moment-to-moment processing of two simple quantity implicatures:  scalar implicatures (avoidance of underinformative statements) and the inference that adjectives will be used contrastively (avoidance on overinformativity). On the basis of the scalars, I will propose that there are two routes by which implicatures are calculated: a slow bottom-up route and top-down route that leads the appearance of instantaneous implicature.  This top-down route relies on the speaker’s conceptualization of the context in linguistically relevant terms. This analysis makes some novel predictions about the role of speaker modelling in the adjective inference.  I’ll present unpublished data that support these new predictions.</p>
            </td>
            <td>Jesse Snedeker</td>
            <td>Zoom</td>
        </tr>
        <tr>
            <td>16:00</td>
            <td>
            Contributed talk: "Efficient Pragmatic Program Synthesis with Informative Specifications"
            <br><br><p style="font-size:0.75rem;">Providing examples is one of the most common way for end-users to interact with program synthesizers. However, program synthesis systems assume that examples consistent with the program are chosen at random, and do not exploit the fact that users choose examples pragmatically. Prior work modeled program synthesis as pragmatic communication, but required an inefficient enumeration of the entire program space. In this paper, we show that it is possible to build a program synthesizer that is both pragmatic and efficient by approximating the joint distribution of programs with a product of independent factors, and performing pragmatic inference on each factor separately. This naive factored distribution approximates the exact joint distribution well when the examples are given pragmatically, and is compatible with a very simple neuro-symbolic synthesis algorithm.</p>
            </td>
            <td>Saujas Vaduguru, Yewen Pu, Kevin Ellis</td>
            <td>Zoom</td>
        </tr>
        <tr>
            <td>16:15</td>
            <td>
            Contributed talk: "Multi-party referential communication in complex strategic games"
            <br><br><p style="font-size:0.75rem;">Verbal communication is an ubiquitous aspect of human interaction occurring in many contexts; however, it is primarily studied in the limited context of two people communicating information. Understanding communication in complex, multi-party interactions is both a scientific challenge for psycholinguistics and an engineering challenge for creating artificial agents who can participate in these richer contexts. We adapted the reference game paradigm to an online 3-player game where players refer to objects in order to coordinate selections based on the available utilities. We ran games with shared or individual payoffs and with or without access to language. Our paradigm can also be used for artificial agents; we trained reinforcement learning-based agents on the same task as a comparison. Our dataset shows the same patterns found in simpler reference games and contains rich language of reference and negotiation.</p>
            </td>
            <td>Jessica Mankewitz, Veronica Boyce, Brandon Waldon, Georgia Loukatou, Dhara Yu, Jesse Mu, Noah Goodman, Michael C Frank</td>
            <td>Zoom</td>
        </tr>
        <tr>
            <td>16:30</td>
            <td>
            Invited talk: TBD
            </td>
            <td>Dan Klein</td>
            <td>Zoom</td>
        </tr>
        <tr>
            <td></td>
            <td>17:00</td>
            <td>Closing remarks</td>
            <td><em>Organizers</em></td>
            <td>Zoom</td>
        </tr>
        <tr class="mingle-row">
            <td></td>
            <td>17:05</td>
            <td>Mingling / Optional poster session</td>
            <td>-</td>
            <td>Gather</td>
        </tr>
    </tbody>
</table>
